1. Hyper-parameters of boosting algorithms
2. XGBoost
3. Implement xgboost
4. Naive Bayes
5. Support Vector Machines
6. Unsupervised learning


1. Hyper - parameters

	a. max_depth = 1, then 2, then 3 ......
		Find the best using GridSearch
	b. max_features
	c. min_samples_leaf
	d. max_leaf_nodes
	e. min_samples_split


2. Gradient Boosting Classifier
	
	Build a GridSearch over these parameters and find the best set of estimators

	a. n_estimators  =  [10, 50, 80, 100, 120]
	b. learning_rate = [0.1, 0.3, 0.7, 1.0]
	c. subsample  = [0.50, 0.55, 0.65, 0.8]
	d. min_samples_leaf = [1, 2, 3]
	e. max_depth = [1, 3, 5, 10]
	f. max_features = [‘auto’, ‘sqrt’, ‘log2’]

	For each print out the n_estimators
	

3. XGBoost Classifier

	a. n_estimators  =  [10, 50, 80, 100, 120]
	b. learning_rate = [0.1, 0.3, 0.7, 1.0]
	c. subsample  = [0.50, 0.55, 0.65, 0.8]
	d. max_depth = [1, 3, 5, 10]
	e. colsample_bytree : [0.5, 0.7, 0.9, 1]
	f. colsample_bynode : 
	g. colsample_bylevel :
	h. missing : 


Exercise : 

	Outlook : Sunny
	Humidity : Normal
	Wind : Strong

	P(P==Y) = Likelihood(Sunny/Y)........
		=	2/9 * 6/9 * 3/9 * 9/14
	P(P==N) = ............
			3/5 * 1/5 * 3/5 * 5/14



Agenda for next week :

1. SVM
2. Clustering
3. Time Series
4. Recommender Systems
5. Attempt the first project ( Mercedez Benz Project )

